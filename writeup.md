Sascha Manalili smanalil@usc.edu

1) For my project, I developed a Convolutional Neural Network to classify facial expressions into seven emotion categories, achieving particularly strong performance in detecting happiness and surprise emotions. The model demonstrates the potential and challenges of automated emotion recognition from visual data.  
2) Dataset: Indicate the dataset you chose to use, any preprocessing steps that were applied, as well as the reasoning behind these choices.  
- I used the FER-2013 dataset, which contains 48x48 pixel grayscale face images categorized into seven emotions:

  \- Training set: 28,709 images

  \- Test set: 3,589 images

  \- Categories: angry, disgust, fear, happy, sad, surprise, and neutral

- The FER-2013 dataset was preprocessed using multiple techniques including grayscale conversion, normalization to center data around 0, and resizing to 48x48 pixels to ensure consistent input dimensions. These preprocessing steps were chosen to reduce computational complexity while maintaining essential facial features, and to help the model converge faster during training through standardized input data.  
3) Model Development and Training: Discuss your model implementation choices, the training procedure, the conditions you settled on (e.g., hyperparameters), and discuss why these are a good set for your task.  
- The implemented model uses a CNN architecture with three convolutional blocks followed by three fully connected layers, trained with Adam optimizer (learning rate=0.001) and CrossEntropyLoss over 10 epochs. This architecture was chosen because it balances model complexity and computational efficiency, with the multiple convolutional layers effectively capturing facial features at different scales, while batch normalization and ReLU activation functions help maintain stable gradients and introduce non-linearity needed for learning complex emotion patterns, ultimately achieving a good balance of training speed and model performance for the facial emotion recognition task.  
4) Model Evaluation/Results: Present the metrics you chose and your model evaluation results.   
- ​​The model's training loss steadily decreased from 1.721 to 0.556 over 10 epochs, showing consistent improvement in performance. The evaluation metrics reveal that the model achieved an overall test accuracy of 56%, with particularly strong performance on 'happy' expressions (79.5% accuracy) and 'surprise' (66.2%), while struggling more with emotions like 'fear' (36.3%) and 'disgust' (39.6%), suggesting the model learned to better recognize more distinctive facial expressions but had difficulty differentiating between similar emotional expressions.  
5) Discussion:   
   1) How well does your dataset, model architecture, training procedures, and chosen metrics fit the task at hand?   
- The dataset and model architecture demonstrate mixed effectiveness for facial emotion recognition, with the FER-2013 dataset providing a solid foundation of 48x48 grayscale images across seven emotion categories, though the uneven performance across emotions suggests some class imbalance issues. The CNN architecture with three convolutional blocks and fully connected layers proved capable of learning distinct emotional features, evidenced by the steadily decreasing training loss from 1.721 to 0.556 over 10 epochs, but the final test accuracy of 56% and particularly low performance on subtle emotions like fear (36.3%) indicates that more sophisticated architectures or training procedures might be needed for capturing more nuanced facial expressions.  
  2) Can your efforts be extended to wider implications, or contribute to social good? Are there any limitations in your methods that should be considered before doing so?  
- This facial emotion recognition technology shows promising potential for social good, particularly in healthcare for mental health monitoring, educational environments for tracking student engagement, and assistive technology for individuals with social processing difficulties, potentially creating more emotionally aware and responsive support systems. However, limitations must be carefully considered before deployment, including privacy concerns regarding continuous emotional monitoring, potential biases in recognition accuracy across different demographics and cultures, and the current model's reliance on controlled conditions which may not translate well to real-world scenarios where lighting, pose, and image quality vary significantly.  
  3) If you were to continue this project, what would be your next steps?  
- To improve this project, I would focus on making the model better at recognizing emotions across different types of people and situations through three main steps. First, I would use data augmentation techniques to create more balanced training data and try using pre-trained models that have already learned basic facial features. Second, I would add more diverse faces to the training data to help the model work better for everyone. Finally, I would test the model in real-world settings to make sure it works well in actual applications, while being careful about privacy and fairness concerns.